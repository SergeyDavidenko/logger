input {
  tcp {
    port => 5008
    codec => json_lines
    type => "logback"
  }
}

# mutate logback appender fields
filter {
  if [type] == "logback" {
    if [stack_trace] =~ /.+/ {
      mutate {
        rename => { "stack_trace" => "StackTrace" }
      }
    }
    mutate {
      rename => {
        "level" => "Severity"
        "thread_name" => "Thread"
      }
    }

    mutate {
      add_field => {
        "logger_info" => "[%{logger_name}]"
        "Time" => "%{@timestamp}"
      }
    }
  }
}

# concatenate message and stacktrace
filter {
  if [StackTrace] =~ /.+/ {
    mutate {
      replace => { "message" => "%{message} %{StackTrace}" }
      remove_field => [ "StackTrace" ]
    }
  }
}

# create file name variable
filter {
  if [serverId] =~ /.+/ { # for gelf
    mutate {
      add_field => {"appid" => "%{serverId}"}
      add_field => {"appname" => "%{serverId}"}
    }
  } else {
    mutate {
      add_field => {
        "appid" => "%{appname}-%{hostip}-%{containerId}"
      }
    }
  }
}

output {
  file {
    path => "/logs/dev/%{appname}/%{+YYYY-MM-dd-HH}-%{hostip}-%{containerId}.log"
    codec => line { format => "%{Time} %{Severity} [%{Thread}] %{logger_info} %{message}" }
    flush_interval => 2
  }
  
  # Отправляем также в Elasticsearch для просмотра в Kibana
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logstash-%{+YYYY.MM.dd}"
  }
  
  # Выводим в stdout для отладки
  stdout {
    codec => rubydebug
  }
}